{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10206472,"sourceType":"datasetVersion","datasetId":6307547}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y\n!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T10:02:40.067056Z","iopub.execute_input":"2025-03-02T10:02:40.067373Z","iopub.status.idle":"2025-03-02T10:05:54.506678Z","shell.execute_reply.started":"2025-03-02T10:02:40.067346Z","shell.execute_reply":"2025-03-02T10:05:54.505394Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True \n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-3B\", # or choose \"unsloth/Llama-3.2-1B\"\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T10:05:54.508128Z","iopub.execute_input":"2025-03-02T10:05:54.508473Z","iopub.status.idle":"2025-03-02T10:06:44.279201Z","shell.execute_reply.started":"2025-03-02T10:05:54.508431Z","shell.execute_reply":"2025-03-02T10:06:44.278252Z"}},"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\nðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.49.0.\n   \\\\   /|    GPU: Tesla P100-PCIE-16GB. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 6.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.35G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"637fddd5825240358df6af4567844007"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/230 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a956704b67643969d091a721efbd25a"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T10:06:44.280877Z","iopub.execute_input":"2025-03-02T10:06:44.281134Z","iopub.status.idle":"2025-03-02T10:06:50.476012Z","shell.execute_reply.started":"2025-03-02T10:06:44.281114Z","shell.execute_reply":"2025-03-02T10:06:50.475312Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.2.15 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"/kaggle/input/desi-hip-hop-lyrics-verses-reverse-prompt/lyrics_described.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T10:06:50.477437Z","iopub.execute_input":"2025-03-02T10:06:50.477756Z","iopub.status.idle":"2025-03-02T10:06:50.628354Z","shell.execute_reply.started":"2025-03-02T10:06:50.477710Z","shell.execute_reply":"2025-03-02T10:06:50.627690Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\n\n# Updated prompt to reflect a rap lyrics generation persona\nrap_alpaca_prompt = \"\"\"You are a master rapper renowned for your innovative lyricism and style. \nBelow is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = \"<|endoftext|>\"  # Update this based on your tokenizer's EOS token\n\ndef format_dataset(df):\n    texts = []\n    for _, row in df.iterrows():\n        instruction = \"Generate a rap verse that matches the given description.\"\n        # Input now includes artist name, title, and reverse prompt\n        input_text = f\"Artist: {row['artist']}\\nTitle: {row['title']}\\nDescription: {row['reverse_prompt']}\"\n        response_text = row[\"verse\"]  # The actual lyrics\n        # Format text similar to the Alpaca dataset using the updated prompt\n        formatted_text = rap_alpaca_prompt.format(instruction, input_text, response_text) + EOS_TOKEN\n        texts.append(formatted_text)\n    \n    return texts\n\n# Apply formatting function\ndf[\"text\"] = format_dataset(df)\n\n# Convert to Hugging Face Dataset\ndataset = Dataset.from_pandas(df[[\"text\"]])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T10:06:50.629191Z","iopub.execute_input":"2025-03-02T10:06:50.629493Z","iopub.status.idle":"2025-03-02T10:06:50.865113Z","shell.execute_reply.started":"2025-03-02T10:06:50.629462Z","shell.execute_reply":"2025-03-02T10:06:50.864440Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    args=TrainingArguments(\n        per_device_train_batch_size=8,  # Increase to 16\n        gradient_accumulation_steps=4,     # Effective batch size = 16 * 4 = 64\n        num_train_epochs=1,\n        warmup_steps=5,\n        learning_rate=2e-4,\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        logging_steps=1,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        output_dir=\"outputs\",\n        report_to=\"none\",\n    ),\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T10:13:50.300544Z","iopub.execute_input":"2025-03-02T10:13:50.300864Z","iopub.status.idle":"2025-03-02T10:14:01.479988Z","shell.execute_reply.started":"2025-03-02T10:13:50.300840Z","shell.execute_reply":"2025-03-02T10:14:01.478963Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Converting train dataset to ChatML (num_proc=2):   0%|          | 0/3550 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"227ff510a0e644e5b4da034e82e1e772"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset (num_proc=2):   0%|          | 0/3550 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71bc28e2d5c940518903545651a4166f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset (num_proc=2):   0%|          | 0/3550 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa1ade584c754acf9b507e003ee45809"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset (num_proc=2):   0%|          | 0/3550 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75f0c64b678047e1a7f8bec879d0a8ed"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T10:14:15.020768Z","iopub.execute_input":"2025-03-02T10:14:15.021223Z","iopub.status.idle":"2025-03-02T10:14:15.029758Z","shell.execute_reply.started":"2025-03-02T10:14:15.021179Z","shell.execute_reply":"2025-03-02T10:14:15.028559Z"}},"outputs":[{"name":"stdout","text":"GPU = Tesla P100-PCIE-16GB. Max memory = 15.888 GB.\n15.494 GB of memory reserved.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T10:14:15.781846Z","iopub.execute_input":"2025-03-02T10:14:15.782155Z","iopub.status.idle":"2025-03-02T13:13:10.260311Z","shell.execute_reply.started":"2025-03-02T10:14:15.782133Z","shell.execute_reply":"2025-03-02T13:13:10.259401Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 3,550 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 32 | Total steps = 111\n \"-____-\"     Number of trainable parameters = 24,313,856\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='111' max='111' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [111/111 2:57:14, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.787300</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.623300</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.684000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.577500</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.476900</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.202800</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>2.454600</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>2.233800</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>2.360200</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>2.009700</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>2.208600</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>2.314500</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>2.149200</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>2.031700</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.908800</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.894600</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>1.906100</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.921900</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>2.032000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.902200</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>2.110300</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>2.215100</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>1.803400</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>2.115400</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>1.967900</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>1.796800</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>1.839600</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>1.846800</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>1.584100</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.839700</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>1.870200</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>1.929700</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>1.990300</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>1.779000</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>1.858100</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>1.730600</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>1.880100</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>1.816800</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>1.849200</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.004200</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>1.604000</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>1.975900</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>1.883600</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>1.888200</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>1.879300</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>1.853300</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>1.798300</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>2.013000</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>1.906000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.042000</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>1.779400</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>1.819400</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>1.622900</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>1.728400</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>1.602000</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>1.899100</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>1.665800</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>1.890400</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>1.900200</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.801900</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>1.960800</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>1.923300</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>1.859800</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>1.921700</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>1.661700</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>1.765300</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>1.658900</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>1.625300</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>1.570100</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.727500</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>1.742300</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>1.561000</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>1.980900</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>1.791700</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.705200</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>2.242000</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>2.095400</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>1.798200</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>1.590300</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.878800</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>1.876200</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>1.587600</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>2.035600</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>1.659400</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>1.709500</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>1.597900</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>1.735100</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>1.753600</td>\n    </tr>\n    <tr>\n      <td>89</td>\n      <td>1.921100</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.677100</td>\n    </tr>\n    <tr>\n      <td>91</td>\n      <td>1.662400</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>1.797900</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>1.738500</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>1.633500</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>1.739800</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>1.736300</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>1.933000</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>2.045900</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>1.478600</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.671400</td>\n    </tr>\n    <tr>\n      <td>101</td>\n      <td>1.518000</td>\n    </tr>\n    <tr>\n      <td>102</td>\n      <td>1.838000</td>\n    </tr>\n    <tr>\n      <td>103</td>\n      <td>1.809400</td>\n    </tr>\n    <tr>\n      <td>104</td>\n      <td>1.724400</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>1.762400</td>\n    </tr>\n    <tr>\n      <td>106</td>\n      <td>1.793500</td>\n    </tr>\n    <tr>\n      <td>107</td>\n      <td>1.681900</td>\n    </tr>\n    <tr>\n      <td>108</td>\n      <td>1.564200</td>\n    </tr>\n    <tr>\n      <td>109</td>\n      <td>1.614300</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.311800</td>\n    </tr>\n    <tr>\n      <td>111</td>\n      <td>1.738300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"model.save_pretrained(\"lora_model\") # Local saving\ntokenizer.save_pretrained(\"lora_model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T13:13:10.261601Z","iopub.execute_input":"2025-03-02T13:13:10.261933Z","iopub.status.idle":"2025-03-02T13:13:10.978791Z","shell.execute_reply.started":"2025-03-02T13:13:10.261901Z","shell.execute_reply":"2025-03-02T13:13:10.978027Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"('lora_model/tokenizer_config.json',\n 'lora_model/special_tokens_map.json',\n 'lora_model/tokenizer.json')"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\n\n# Enable faster inference\nFastLanguageModel.for_inference(model)\n\n\ndef generate_rap_lyrics(description, artist_name=\"Unknown Artist\", song_title=\"New Track\", language=\"English\"):\n    # Modified instruction to include language preference\n    instruction = f\"Generate a rap verse in {language} that matches the given description.\"\n    input_text = f\"Artist: {artist_name}\\nTitle: {song_title}\\nDescription: {description}\\nLanguage: {language}\"\n    \n    prompt = rap_alpaca_prompt.format(instruction, input_text, \"\")\n    \n    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n    \n    from transformers import TextStreamer\n    text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n    \n    generated_ids = model.generate(\n        input_ids=inputs.input_ids, \n        attention_mask=inputs.attention_mask,\n        streamer=text_streamer, \n        max_new_tokens=256,         # Longer output for complete verses\n        temperature=0.5,            # Add some creativity\n        top_p=0.5,                  # Nucleus sampling for more diverse outputs\n        repetition_penalty=1.2,     # Add repetition penalty to avoid repeated phrases\n        no_repeat_ngram_size=3,     # Avoid repeating 3-grams\n        do_sample=True,             # Enable sampling\n        pad_token_id=tokenizer.eos_token_id\n    )\n    \n    # Get generated text without the prompt\n    generated_text = tokenizer.decode(generated_ids[0, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n    return generated_text\n\n# Example usage - Hindi verse with Mumbai references\ndescription = \"A gritty street narrative with references to Mumbai streets and hustle culture\"\nartist_name = \"Divine\"\nsong_title = \"Gully Flow\"\n\nprint(\"\\nðŸŽµ Generated Hindi Rap Verse ðŸŽµ\\n\")\ngenerate_rap_lyrics(description, artist_name, song_title, language=\"Hindi\")\n\n# Example with Hinglish option\ndescription = \"A motivational anthem about rising from struggles and achieving success despite obstacles\"\nartist_name = \"Raftaar\"\nsong_title = \"Rise Up\"\n\nprint(\"\\nðŸŽµ Generated Hinglish Rap Verse ðŸŽµ\\n\")\ngenerate_rap_lyrics(description, artist_name, song_title, language=\"Hinglish\")\n\n# Example with pure Hindi lyrics\ndescription = \"A song about youth culture and social change in modern India\"\nartist_name = \"MC Altaf\"\nsong_title = \"Naya Zamana\"\n\nprint(\"\\nðŸŽµ Generated Pure Hindi Rap Verse ðŸŽµ\\n\")\ngenerate_rap_lyrics(description, artist_name, song_title, language=\"Pure Hindi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T13:39:26.401280Z","iopub.execute_input":"2025-03-02T13:39:26.401574Z"}},"outputs":[{"name":"stdout","text":"\nðŸŽµ Generated Hindi Rap Verse ðŸŽµ\n\nGully flow gully flow\nMumbai mein chal rhaa huun maine kuchh bhi nahiin kiya\nKabhi kabhi mujhe lagtaa hai jaise maiyaa kaanpataa huaa\nAur jab bhee meraa naam sunaate tou merey saath meiin sabko jaantaa hain\nBhai logon ko parr bhoolnaa naa padega\nJaanke merrie gaane aur tere liye khud seey banake baat karuun\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}